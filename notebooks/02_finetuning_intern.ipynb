{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32eebc1",
   "metadata": {},
   "source": [
    "### 1. Environment Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Environment Detection\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# 2. Hardware Validation \n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"\n",
    "is_t4 = \"T4\" in gpu_name\n",
    "\n",
    "# 3. Status Report\n",
    "print(f\"üåç Is Google Colab: {IN_COLAB}\")\n",
    "print(f\"üöÄ GPU Detected: {gpu_name}\")\n",
    "\n",
    "if IN_COLAB and is_t4:\n",
    "    print(\"‚úÖ Success: Environment meets T4 GPU requirements.\")\n",
    "else:\n",
    "    print(\"‚ùå Check Failed: Ensure T4 GPU runtime is enabled in Notebook Settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the path of current directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb02b97",
   "metadata": {},
   "source": [
    "### 2. Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Install Unsloth & Core Ecosystem\n",
    "try:\n",
    "    import unsloth\n",
    "    print(\"‚úÖ Unsloth already installed.\")\n",
    "except ImportError:\n",
    "    # Optimized install for T4 GPU Runtimes\n",
    "    !pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install -q --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "    print(\"‚úÖ Dependencies installed successfully.\")\n",
    "\n",
    "# Force update to ensure latest patches for Llama-3-8b support\n",
    "!pip install -q --upgrade --no-cache-dir unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e8a35",
   "metadata": {},
   "source": [
    "### 3. Workspace Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158da3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# Project Artifacts Folder ID\n",
    "folder_id = '1lBF3jieW1m4dapjzokIH1nTnHnBhkUQL'\n",
    "drive_url = f'https://drive.google.com/drive/folders/{folder_id}'\n",
    "\n",
    "# Download the synchronized project folder (uploaded the same to my gdrive account) from gdrive\n",
    "# 'remaining_ok=True' ensures robustness for larger artifact transfers\n",
    "gdown.download_folder(drive_url, quiet=False, remaining_ok=True)\n",
    "\n",
    "# Validation of required directory structure\n",
    "if os.path.exists(\"MiniProject01\"):\n",
    "    print(\"‚úÖ Workspace synchronized. Ready for model loading.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Folder name mismatch. Check gdown output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b328fc",
   "metadata": {},
   "source": [
    "### 4. Configuration and Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950aa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import gc\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# 1. Load Centralized Project Settings \n",
    "with open(\"MiniProject01/src/config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 2. Load Instruction Persona and Prompts \n",
    "with open(\"MiniProject01/src/config/prompts.yaml\", \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "# 3. Status Report\n",
    "print(\"‚úÖ Configuration and Prompt Library Loaded.\")\n",
    "print(f\"   Target Model: {config['finetuning']['model_name']}\")\n",
    "print(f\"   Training Limit: {config['finetuning']['training']['max_steps']} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29eff9",
   "metadata": {},
   "source": [
    "### 5. Model Loading & LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Memory Management: Clear VRAM for the T4 GPU \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ft_conf = config['finetuning']\n",
    "\n",
    "# 2. Load Base Model with 4-bit Quantization\n",
    "print(\"‚è≥ Loading Model via Unsloth (4-bit NF4)...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = ft_conf['model_name'],\n",
    "    max_seq_length = ft_conf['max_seq_length'],\n",
    "    dtype = None,\n",
    "    load_in_4bit = ft_conf['load_in_4bit'], # Mandatory for T4 fit\n",
    "    device_map = {\"\": 0} # Explicitly map to the single T4 GPU\n",
    ")\n",
    "\n",
    "# 3. Add LoRA Adapters for Projection Modules\n",
    "print(\"‚öôÔ∏è Injecting LoRA Adapters into q, k, v, o_proj...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = ft_conf['lora']['r'],\n",
    "    target_modules = ft_conf['lora']['target_modules'], # q_proj, k_proj, v_proj, o_proj\n",
    "    lora_alpha = ft_conf['lora']['alpha'],\n",
    "    lora_dropout = ft_conf['lora']['dropout'],\n",
    "    bias = ft_conf['lora']['bias'],\n",
    "    use_gradient_checkpointing = \"unsloth\", # Memory-efficient gradient handling\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model Ready for Financial Intelligence Training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a2425",
   "metadata": {},
   "source": [
    "### 6. Dataset Preparation & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Synthetic Training Data \n",
    "train_data_path = \"/content/MiniProject01/artifacts/data/train.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
    "\n",
    "# 2. Define Chat Template Mapping\n",
    "def format_chat_template(row):\n",
    "    # Retrieve the Lead AI Architect persona\n",
    "    system_msg = prompts['intern_persona'].strip()\n",
    "    \n",
    "    # Combine question and Uber report context \n",
    "    user_content = f\"{row['instruction']}\\n\\nContext:\\n{row['input']}\"\n",
    "    \n",
    "    # Structure for Llama-3 instruction tuning\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": row['output']}\n",
    "    ]\n",
    "    \n",
    "    # Apply tokenizer's chat template without tokenizing yet\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "print(\"üìù Formatting Dataset into Instruction Blocks...\")\n",
    "# Process in parallel for speed\n",
    "dataset = dataset.map(format_chat_template, num_proc=2)\n",
    "\n",
    "print(f\"‚úÖ Processed {len(dataset)} instruction-tuning examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91228b8",
   "metadata": {},
   "source": [
    "### 7. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_conf = config['finetuning']['training']\n",
    "\n",
    "# 1. Configure Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = tr_conf['output_dir'],\n",
    "    per_device_train_batch_size = tr_conf['batch_size'],\n",
    "    gradient_accumulation_steps = tr_conf['grad_accum_steps'],\n",
    "    warmup_steps = 10,\n",
    "    max_steps = tr_conf['max_steps'], \n",
    "    learning_rate = float(tr_conf['learning_rate']),\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = tr_conf['optim'], # adamw_8bit for T4 VRAM efficiency\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = tr_conf['seed'],\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "# 2. Initialize SFTTrainer\n",
    "print(\"üöÄ Initializing SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = config['finetuning']['max_seq_length'],\n",
    "    args = training_args,\n",
    "    packing = False, # Standard for single-instruction blocks\n",
    ")\n",
    "\n",
    "# 3. Execute Training Loop\n",
    "print(\"üî• Starting The Intern Fine-Tuning...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"‚úÖ Training Complete! Loss converged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9bf8ed",
   "metadata": {},
   "source": [
    "### 8. Inference Pipeline & Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f397a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Optimize for Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# 2. Define Lead Architect Inference Function\n",
    "def query_intern(instruction, context=\"\"):\n",
    "    \"\"\"Generates a response using the fine-tuned 'Intern' persona.\"\"\"\n",
    "    system_msg = prompts['intern_persona'].strip()\n",
    "    user_content = f\"{instruction}\\n\\nContext:\\n{context}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    # Apply the specific Llama-3 chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output using config-driven parameters\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=config['finetuning']['inference']['max_new_tokens'],\n",
    "        use_cache=config['finetuning']['inference']['use_cache'],\n",
    "        temperature=config['finetuning']['inference']['temperature'],\n",
    "    )\n",
    "    \n",
    "    # Extract response only\n",
    "    response = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0]\n",
    "    return response.strip()\n",
    "\n",
    "# 3. Load Golden Test Set for Evaluation\n",
    "test_data_path = \"/content/MiniProject01/artifacts/data/golden_test_set.jsonl\"\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 4. Execute Prediction Loop with Latency Tracking\n",
    "results = []\n",
    "print(f\"‚ö° Generating predictions for {len(test_data)} items...\")\n",
    "\n",
    "for item in tqdm(test_data):\n",
    "    q = item['instruction']\n",
    "    \n",
    "    # Precise Latency Measurement\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        pred = query_intern(q, item['input'])\n",
    "    except Exception as e:\n",
    "        pred = f\"Inference Error: {str(e)}\"\n",
    "    end_time = time.time()\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": item['output'],\n",
    "        \"intern_answer\": pred,\n",
    "        \"intern_latency_ms\": (end_time - start_time) * 1000\n",
    "    })\n",
    "\n",
    "# 5. Save Artifacts for The Showdown\n",
    "intern_preds_path = \"/content/MiniProject01/artifacts/data/intern_predictions.jsonl\"\n",
    "with open(intern_preds_path, \"w\") as f:\n",
    "    for entry in results:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Success: Predictions saved to {intern_preds_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725e627",
   "metadata": {},
   "source": [
    "### 9. Adapter Preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Output Directory\n",
    "adapter_path = \"/content/MiniProject01/artifacts/outputs/llama-3-financial-intern\"\n",
    "\n",
    "# 2. Save Trained LoRA Adapters\n",
    "# This saves only the incremental weights, making it much smaller than the full model\n",
    "model.save_pretrained(adapter_path)\n",
    "\n",
    "# 3. Save Tokenizer for Consistency\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"‚úÖ Adapters and tokenizer successfully saved to: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e047f4",
   "metadata": {},
   "source": [
    "### 10. Artifacts Export & Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf526514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive Mounted at: /content/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a80e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Define Persistent Storage Paths\n",
    "source_folder = \"/content/MiniProject01\"\n",
    "\n",
    "# Destination: Google Drive directory for the final artifacts\n",
    "destination_folder = \"/content/drive/MyDrive/MiniProject01\"\n",
    "\n",
    "# 2. Create the destination directory structure\n",
    "print(f\"üìÇ Creating persistent directory: {destination_folder}\")\n",
    "!mkdir -p \"$destination_folder\"\n",
    "\n",
    "# 3. Synchronize Artifacts\n",
    "if os.path.exists(source_folder):\n",
    "    print(f\"üöÄ Archiving adapters and predictions to Drive...\")\n",
    "    # Copying recursively to preserve folder structure\n",
    "    !cp -r \"$source_folder\"/* \"$destination_folder\"/\n",
    "    \n",
    "    # 4. Final Audit of Backed-up Files\n",
    "    print(\"\\n‚úÖ Backup Complete. Files in Drive:\")\n",
    "    !ls -lh \"$destination_folder\"\n",
    "else:\n",
    "    print(f\"‚ùå Error: Source folder '{source_folder}' not found. Check local paths.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
