{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ce5aaf",
   "metadata": {},
   "source": [
    "### 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, random\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from environment variables\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Add the project root to path for internal service imports\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "from src.services.llm_services import load_config, query_broker, clean_json_output, load_prompts, format_prompt\n",
    "from src.services.data_manager import DataManager\n",
    "\n",
    "# Initialize global configuration and prompt library\n",
    "cfg = load_config(\"../src/config/config.yaml\")\n",
    "prompts_lib = load_prompts(\"../src/config/prompts.yaml\")\n",
    "\n",
    "print(f\"âœ… Data Factory Initialized. Target PDF: {cfg['paths']['pdf_source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff19d4",
   "metadata": {},
   "source": [
    "### 2. Ingestion & Chunking Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestion\n",
    "raw_text = DataManager.extract_and_clean_pdf(cfg['paths']['pdf_source'])\n",
    "\n",
    "# Chunking Alignment\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=cfg['chunking']['size'], \n",
    "    chunk_overlap=cfg['chunking']['overlap'],\n",
    "    separators=[cfg['chunking']['separator']] \n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(raw_text)\n",
    "\n",
    "print(f\"ðŸ“Š PDF Processing Complete.\")\n",
    "print(f\"   - Total Chunks: {len(chunks)}\")\n",
    "print(f\"   - Character Count of Raw Text: {len(raw_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672eb4e8",
   "metadata": {},
   "source": [
    "### 3. The Generation Loop (Dual-LLM Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e42571",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa_pairs = []\n",
    "\n",
    "print(f\"ðŸš€ Starting Data Factory for {len(chunks)} chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(tqdm(chunks, desc=\"Processing Uber Annual Report\")):\n",
    "    try:\n",
    "        # Step A: Question Generation (LLM A)\n",
    "        # The prompt is instructed to cover Hard Facts, Strategy, and Tone\n",
    "        q_prompt = format_prompt(\"teacher_questions\", prompts_lib, chunk_text=chunk)\n",
    "        raw_q = query_broker(cfg, q_prompt, role=\"llm_a\", format_json=True)\n",
    "        \n",
    "        if not raw_q:\n",
    "            continue \n",
    "        \n",
    "        questions = json.loads(clean_json_output(raw_q))\n",
    "        if isinstance(questions, dict): \n",
    "            questions = list(questions.values())[0]\n",
    "        \n",
    "        if not isinstance(questions, list): \n",
    "            continue\n",
    "\n",
    "        # Step B: Answer Generation (LLM B) \n",
    "        for q in questions[:10]: # Limit to 10 pairs per chunk \n",
    "            a_prompt = format_prompt(\"student_answers\", prompts_lib, chunk_text=chunk, question=q)\n",
    "            answer = query_broker(cfg, a_prompt, role=\"llm_b\")\n",
    "            \n",
    "            if answer and \"Information not available\" not in answer:\n",
    "                # Structuring expected json format\n",
    "                all_qa_pairs.append({\n",
    "                    \"instruction\": q,\n",
    "                    \"input\": chunk,\n",
    "                    \"output\": answer.strip(),\n",
    "                    \"category\": \"financial_report\",\n",
    "                    \"metadata\": {\n",
    "                        \"global_id\": len(all_qa_pairs) + 1,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"sequential_order\": True\n",
    "                    }\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        # Prevents a single parsing error from stopping the whole process\n",
    "        continue\n",
    "\n",
    "print(f\"âœ… Generation Complete. Total Pairs Created: {len(all_qa_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b444f0",
   "metadata": {},
   "source": [
    "### 4. Storage & Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528288b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to ensure categorical variety in both sets\n",
    "random.shuffle(all_qa_pairs)\n",
    "\n",
    "# Storage & Splitting: 80% Train / 20% Golden Test \n",
    "split_point = int(len(all_qa_pairs) * cfg['generation'].get('split_ratio', 0.8))\n",
    "train_slice = all_qa_pairs[:split_point]\n",
    "test_slice = all_qa_pairs[split_point:]\n",
    "\n",
    "def save_as_jsonl(dataset, file_path):\n",
    "    \"\"\"Saves the dataset in JSONL format for the Fine-Tuning pipeline.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in dataset:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Save output artifacts\n",
    "save_as_jsonl(train_slice, cfg['paths']['train_data'])   # train.jsonl\n",
    "save_as_jsonl(test_slice, cfg['paths']['test_data'])    # golden_test_set.jsonl\n",
    "\n",
    "print(f\"ðŸ’¾ Files Saved Successfully.\")\n",
    "print(f\"   - Training Data: {len(train_slice)} items saved to {cfg['paths']['train_data']}\")\n",
    "print(f\"   - Golden Test Set: {len(test_slice)} items saved to {cfg['paths']['test_data']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
