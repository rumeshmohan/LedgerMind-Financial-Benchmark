{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb56eb1",
   "metadata": {},
   "source": [
    "### 1. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c263668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import requests\n",
    "import weaviate\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from weaviate.classes.init import Auth, AdditionalConfig, Timeout\n",
    "from sentence_transformers import CrossEncoder\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24b448",
   "metadata": {},
   "source": [
    "### 2. Configuration and Prompt Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff032d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = \"../src/config/\"\n",
    "\n",
    "# Load Configuration\n",
    "with open(os.path.join(CONFIG_DIR, \"config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load Prompts\n",
    "with open(os.path.join(CONFIG_DIR, \"prompts.yaml\"), \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "print(f\"âœ… Config & Prompts Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971aabd",
   "metadata": {},
   "source": [
    "### 3. Judge Functions (ROUGE & LLM Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def calculate_rouge(pred, ref):\n",
    "    \"\"\"Calculates ROUGE-L F-measure between prediction and ground truth.\"\"\"\n",
    "    if not pred or pred in [\"N/A\", \"Error\"]: return 0.0\n",
    "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "\n",
    "def llm_judge(question, answer, truth):\n",
    "    \"\"\"Uses an LLM to rate accuracy on a scale of 1-5.\"\"\"\n",
    "    if not answer or answer == \"N/A\": return 1\n",
    "    \n",
    "    provider = config['evaluation']['judge_provider']\n",
    "    settings = config['providers'][provider]\n",
    "    model_name = settings['judge_model']\n",
    "    \n",
    "    prompt = prompts['judge_prompt'].format(\n",
    "        question=question, truth=truth, answer=answer\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        if \"ollama\" in provider:\n",
    "            response = requests.post(settings['base_url'], json={\n",
    "                \"model\": model_name, \"prompt\": prompt, \"stream\": False, \n",
    "                \"options\": {\"temperature\": 0.0}}, timeout=30)\n",
    "            text = response.json()['response'].strip()\n",
    "        else:\n",
    "            api_key = os.getenv(f\"{provider.upper()}_API_KEY\") or os.getenv(\"HF_TOKEN\")\n",
    "            response = requests.post(settings['base_url'], \n",
    "                headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
    "                json={\"model\": model_name, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.0},\n",
    "                timeout=10)\n",
    "            text = response.json()['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        match = re.search(r'\\[\\[([1-5])\\]\\]', text) or re.search(r'\\b([1-5])\\b', text)\n",
    "        return int(match.group(1)) if match else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "print(\"âœ… Evaluation functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68e763",
   "metadata": {},
   "source": [
    "### 4. Weaviate, Reranker Setup & Advanced RAG with RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”Œ Connecting to Weaviate...\")\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.getenv(\"WEAVIATE_URL\"),\n",
    "    auth_credentials=Auth.api_key(os.getenv(\"WEAVIATE_API_KEY\")),\n",
    "    headers={\"X-HuggingFace-Api-Key\": os.getenv(\"HF_TOKEN\")},\n",
    "    additional_config=AdditionalConfig(timeout=Timeout(init=60, query=60))\n",
    ")\n",
    "\n",
    "print(\"ðŸ“š Loading Cross-Encoder Reranker...\")\n",
    "reranker = CrossEncoder(config['weaviate']['reranker_model'])\n",
    "\n",
    "def reciprocal_rank_fusion(ranked_lists: List[List[Tuple[str, any]]], k: int = 60) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Implements Reciprocal Rank Fusion (RRF) algorithm.\n",
    "    \n",
    "    RRF Score Formula: RRF(d) = Î£ 1/(k + rank_i(d))\n",
    "    \n",
    "    Args:\n",
    "        ranked_lists: List of ranked result lists from different retrievers\n",
    "        k: RRF constant parameter (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping doc_id to RRF score and document object\n",
    "    \"\"\"\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    for ranked_list in ranked_lists:\n",
    "        for rank, (doc_id, doc_obj) in enumerate(ranked_list):\n",
    "            if doc_id not in rrf_scores:\n",
    "                rrf_scores[doc_id] = {'score': 0.0, 'doc': doc_obj}\n",
    "            rrf_scores[doc_id]['score'] += 1.0 / (k + rank + 1)\n",
    "    \n",
    "    return rrf_scores\n",
    "\n",
    "def query_librarian(q: str, top_k: int = 10, rrf_k: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    Advanced RAG Pipeline with Reciprocal Rank Fusion.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Dense Vector Search (Semantic)\n",
    "    2. BM25 Keyword Search (Lexical)\n",
    "    3. Reciprocal Rank Fusion\n",
    "    4. Cross-Encoder Reranking\n",
    "    \n",
    "    Args:\n",
    "        q: Query string\n",
    "        top_k: Number of candidates per retriever\n",
    "        rrf_k: RRF constant\n",
    "    \n",
    "    Returns:\n",
    "        Most relevant document content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        col = client.collections.get(config['weaviate']['class_name'])\n",
    "        \n",
    "        # Step 1: Dense Vector Search\n",
    "        vector_results = col.query.near_text(query=q, limit=top_k * 2)\n",
    "        \n",
    "        # Step 2: BM25 Keyword Search\n",
    "        bm25_results = col.query.bm25(query=q, limit=top_k * 2)\n",
    "        \n",
    "        # Check if we have results\n",
    "        if not vector_results.objects and not bm25_results.objects:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        # Step 3: Apply Reciprocal Rank Fusion\n",
    "        vector_ranked = [(str(obj.uuid), obj) for obj in vector_results.objects]\n",
    "        bm25_ranked = [(str(obj.uuid), obj) for obj in bm25_results.objects]\n",
    "        \n",
    "        rrf_scores = reciprocal_rank_fusion(\n",
    "            ranked_lists=[vector_ranked, bm25_ranked],\n",
    "            k=rrf_k\n",
    "        )\n",
    "        \n",
    "        # Sort by RRF score\n",
    "        rrf_sorted = sorted(\n",
    "            rrf_scores.items(),\n",
    "            key=lambda x: x[1]['score'],\n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        # Extract candidates\n",
    "        rrf_candidates = [doc_data['doc'] for doc_id, doc_data in rrf_sorted]\n",
    "        candidate_texts = [obj.properties['content'] for obj in rrf_candidates]\n",
    "        \n",
    "        # Step 4: Cross-Encoder Reranking\n",
    "        pairs = [[q, text] for text in candidate_texts]\n",
    "        scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Return best match\n",
    "        return candidate_texts[scores.argmax()]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error in query_librarian: {str(e)}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "print(\"âœ… Librarian Online with RRF Pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arena_cell",
   "metadata": {},
   "source": [
    "### 5. Evaluation Arena - The Showdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "final_output_path = config['paths']['final_output_path']\n",
    "checkpoint_path = config['paths']['checkpoint_path']\n",
    "\n",
    "# Load The Intern's predictions\n",
    "print(\"ðŸ“‚ Loading The Intern's predictions...\")\n",
    "with open(config['paths']['intern_preds'], \"r\") as f:\n",
    "    intern_lookup = {item['question']: item for item in [json.loads(line) for line in f]}\n",
    "\n",
    "# Load golden test set\n",
    "print(\"ðŸ“‚ Loading golden test set...\")\n",
    "with open(config['paths']['test_data'], \"r\") as f:\n",
    "    test_set = [json.loads(line) for line in f] \n",
    "\n",
    "# Initialize results tracking\n",
    "results = []\n",
    "processed_questions = set()\n",
    "\n",
    "# Resume from checkpoint if exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    existing_df = pd.read_csv(checkpoint_path)\n",
    "    results = existing_df.to_dict('records')\n",
    "    processed_questions = set(existing_df[\"Question\"].tolist())\n",
    "    print(f\"ðŸ”„ Resuming from checkpoint. Skipping {len(processed_questions)} items.\")\n",
    "\n",
    "print(f\"\\nâš”ï¸ Starting The Showdown ({len(test_set)} items total)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Arena Rules:\")\n",
    "print(\"  â€¢ The Librarian: Advanced RAG with RRF + Cross-Encoder Reranking\")\n",
    "print(\"  â€¢ The Intern: Fine-Tuned Llama-3-8B with LoRA\")\n",
    "print(\"  â€¢ Metrics: ROUGE-L, LLM-as-Judge (1-5), Latency (ms)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Main evaluation loop\n",
    "for item in tqdm(test_set, desc=\"Evaluating\"):\n",
    "    q = item['instruction']\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if q in processed_questions:\n",
    "        continue\n",
    "    \n",
    "    truth = item['output']\n",
    "    \n",
    "    #The Librarian's Turn\n",
    "    start = time.time()\n",
    "    try:\n",
    "        lib_ans = query_librarian(q)\n",
    "    except Exception as e:\n",
    "        lib_ans = \"Error\"\n",
    "        print(f\"\\nâš ï¸ Librarian error on question: {q[:50]}...\")\n",
    "    lib_lat = (time.time() - start) * 1000\n",
    "    \n",
    "    #The Intern's Turn\n",
    "    rec = intern_lookup.get(q, {})\n",
    "    int_ans = rec.get('intern_answer', \"N/A\")\n",
    "    int_lat = rec.get('intern_latency_ms', 0.0)\n",
    "    \n",
    "    #Calculate Metrics\n",
    "    l_rouge = calculate_rouge(lib_ans, truth)\n",
    "    i_rouge = calculate_rouge(int_ans, truth)\n",
    "    l_score = llm_judge(q, lib_ans, truth)\n",
    "    i_score = llm_judge(q, int_ans, truth)\n",
    "    \n",
    "    #Record Results\n",
    "    res_row = {\n",
    "        \"Question\": q,\n",
    "        \"Librarian_Judge\": l_score,\n",
    "        \"Intern_Judge\": i_score,\n",
    "        \"Librarian_Rouge\": l_rouge,\n",
    "        \"Intern_Rouge\": i_rouge,\n",
    "        \"Librarian_Latency\": lib_lat,\n",
    "        \"Intern_Latency\": int_lat\n",
    "    }\n",
    "    results.append(res_row)\n",
    "    \n",
    "    # Save checkpoint after each item\n",
    "    pd.DataFrame([res_row]).to_csv(\n",
    "        checkpoint_path,\n",
    "        mode='a',\n",
    "        header=not os.path.isfile(checkpoint_path),\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Rate limiting to avoid API throttling\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save final results\n",
    "df_final = pd.DataFrame(results)\n",
    "df_final.to_csv(final_output_path, index=False)\n",
    "\n",
    "# Clean up checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    os.remove(checkpoint_path)\n",
    "\n",
    "print(f\"\\nâœ… Evaluation Complete! Results saved to {final_output_path}\")\n",
    "print(f\"ðŸ“Š Total items evaluated: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_summary",
   "metadata": {},
   "source": [
    "### 6. Results Summary & Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_display",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load results\n",
    "if os.path.exists(final_output_path):\n",
    "    report_df = pd.read_csv(final_output_path)\n",
    "    status = f\"âœ… Final Showdown Report ({len(report_df)} Items)\"\n",
    "elif os.path.exists(checkpoint_path):\n",
    "    report_df = pd.read_csv(checkpoint_path)\n",
    "    status = \"âš ï¸ Partial Showdown Report (In Progress)\"\n",
    "else:\n",
    "    report_df = None\n",
    "    print(\"âŒ No results found. Please run the Evaluation Arena loop in Cell 5 first.\")\n",
    "\n",
    "if report_df is not None:\n",
    "    print(f\"--- {status} ---\")\n",
    "    print(f\"Evaluation Pipeline: Vector Search â†’ BM25 â†’ RRF â†’ Cross-Encoder Reranking\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate wins\n",
    "    lib_wins = (report_df['Librarian_Judge'] > report_df['Intern_Judge']).sum()\n",
    "    int_wins = (report_df['Intern_Judge'] > report_df['Librarian_Judge']).sum()\n",
    "    ties = (report_df['Librarian_Judge'] == report_df['Intern_Judge']).sum()\n",
    "\n",
    "    # Create summary table\n",
    "    summary_data = {\n",
    "        \"Metric\": [\n",
    "            \"Avg Judge Score (1-5)\",\n",
    "            \"Avg ROUGE-L (0-1)\",\n",
    "            \"Avg Latency (ms)\",\n",
    "            \"Total Wins (Judge)\"\n",
    "        ],\n",
    "        \"Librarian (RAG+RRF)\": [\n",
    "            round(report_df['Librarian_Judge'].mean(), 2),\n",
    "            round(report_df.get('Librarian_Rouge', 0).mean(), 4),\n",
    "            f\"{round(report_df['Librarian_Latency'].mean(), 2)} ms\",\n",
    "            lib_wins\n",
    "        ],\n",
    "        \"Intern (Fine-Tuned)\": [\n",
    "            round(report_df['Intern_Judge'].mean(), 2),\n",
    "            round(report_df.get('Intern_Rouge', 0).mean(), 4),\n",
    "            f\"{round(report_df['Intern_Latency'].mean(), 2)} ms\",\n",
    "            int_wins\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nðŸ† Consolidated Performance Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(metrics_df.rename(index=lambda x: x + 1))\n",
    "    \n",
    "    print(f\"\\nTotal Items Evaluated: {len(report_df)} | Total Ties: {ties}\")\n",
    "    \n",
    "    # Determine winner\n",
    "    if int_wins > lib_wins:\n",
    "        winner = \"ðŸ† The Intern (Fine-Tuned Model)\"\n",
    "        margin = int_wins - lib_wins\n",
    "    elif lib_wins > int_wins:\n",
    "        winner = \"ðŸ† The Librarian (RAG with RRF)\"\n",
    "        margin = lib_wins - int_wins\n",
    "    else:\n",
    "        winner = \"ðŸ¤ TIE\"\n",
    "        margin = 0\n",
    "    \n",
    "    print(f\"\\n{winner}\")\n",
    "    if margin > 0:\n",
    "        print(f\"Win Margin: {margin} questions ({margin/len(report_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Detailed Data Sample (Top 5):\")\n",
    "    display(report_df.head(5).rename(index=lambda x: x + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf10df",
   "metadata": {},
   "source": [
    "### 7. BONUS: Monthly Cloud Cost Analysis\n",
    "\n",
    "To provide a realistic financial comparison, this analysis estimates the monthly operational costs for serving both **\"The Intern\" (Fine-Tuning)** and **\"The Librarian\" (Advanced RAG with RRF)** architectures.\n",
    "\n",
    "## Scale Assumptions\n",
    "\n",
    "- **Daily Active Users (DAU)**: 500\n",
    "- **Queries per User**: 10\n",
    "- **Total Daily Queries**: 5,000\n",
    "- **Total Monthly Volume**: 150,000 queries (30-day month)\n",
    "\n",
    "---\n",
    "\n",
    "## Strategy A: \"The Intern\" (Parametric Memory)\n",
    "\n",
    "This strategy relies on serving the fine-tuned Llama-3-8b model. To maintain consistent performance for 500 users, a dedicated GPU instance is required to host the model 24/7.\n",
    "\n",
    "| Component       | Service / Hardware              | Unit Cost (Est. 2026) | Monthly Cost |\n",
    "|-----------------|--------------------------------|-----------------------|--------------|\n",
    "| Model Hosting   | AWS EC2 g4dn.xlarge (NVIDIA T4) | $0.5260 / hr         | $383.98      |\n",
    "| Data Storage    | Amazon EBS (125 GB NVMe SSD)    | Included in instance | $0.00        |\n",
    "| Update Cost     | Compute for retraining (Min. 100 steps) | ~$25.00 / run | ~$25.00      |\n",
    "| **Total Monthly** |                               |                       | **~$408.98** |\n",
    "\n",
    "**Note**: While hosting is fixed, \"The Intern\" suffers from **parametric decay**. Each new financial report requires a full retraining cycle, adding compute costs and high engineering effort.\n",
    "\n",
    "---\n",
    "\n",
    "## Strategy B: \"The Librarian\" (Non-Parametric Memory with RRF)\n",
    "\n",
    "The Advanced RAG strategy uses a modular infrastructure with RRF-enhanced retrieval. While it has more components, it allows for **real-time data updates** without the need for retraining.\n",
    "\n",
    "| Component       | Service / Component             | Unit Cost (Est. 2026)     | Monthly Cost |\n",
    "|-----------------|--------------------------------|---------------------------|--------------|\n",
    "| Vector Database | Weaviate Cloud (Standard)      | $25.00 base + usage      | ~$35.00      |\n",
    "| Embeddings      | Snowflake Arctic (150k queries) | $0.025 / 1M tokens       | ~$5.00       |\n",
    "| Reranker        | Cross-Encoder (Hosted t3.medium) | $0.0416 / hr           | ~$30.00      |\n",
    "| Base LLM Hosting | AWS EC2 g4dn.xlarge (Standard Llama-3) | $0.5260 / hr    | $383.98      |\n",
    "| **Total Monthly** |                               |                           | **~$453.98** |\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Comparison Summary\n",
    "\n",
    "| Metric                | \"The Intern\" (Fine-Tuning) | \"The Librarian\" (RAG+RRF) |\n",
    "|-----------------------|----------------------------|---------------------------|\n",
    "| **Total Monthly Cost** | $408.98                   | $453.98                   |\n",
    "| **Cost Per Query**     | $0.0027                   | $0.0030                   |\n",
    "| **Data Update Cost**   | High (Requires full retraining) | Low (Real-time indexing) |\n",
    "| **Factual Grounding**  | Low (Risk of Hallucination) | High (Direct Citations + RRF) |\n",
    "\n",
    "---\n",
    "\n",
    "## Economic Verdict\n",
    "\n",
    "- **Short-Term Savings**: \"The Intern\" appears roughly **10-11% cheaper** in fixed monthly infrastructure costs.\n",
    "  \n",
    "- **Long-Term Value**: \"The Librarian\" with RRF is the **superior choice for Alpha-Yield Capital**. It avoids the recurring **\"engineering tax\"** and compute costs of retraining every time a new financial report is released, providing exact page citations and real-time data freshness essential for fintech applications.\n",
    "\n",
    "- **RRF Advantage**: The Reciprocal Rank Fusion component adds negligible compute cost (<$5/month) but significantly improves retrieval precision for financial queries requiring both semantic understanding and exact entity matching.\n",
    "\n",
    "**Bottom Line**: While fine-tuning may seem appealing on paper, the hidden costs of parametric decay, retraining cycles, and hallucination risks make Advanced RAG with RRF the more economically sound and operationally reliable solution for production financial systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
