# Operation Ledger-Mind: Configuration

# LLM Providers
providers:
  ollama:
    model: "gemma3:4b"
    judge_model: "llama3.2:latest"
    base_url: "http://localhost:11434/api/generate"
  
  groq:
    model: "llama-3.3-70b-versatile"
    judge_model: "llama-3.3-70b-versatile"
    base_url: "https://api.groq.com/openai/v1/chat/completions"
  
  google:
    model: "gemini-2.5-flash-lite"
    judge_model: "gemini-2.5-flash-lite"
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
  
  openrouter:
    llm_a_model: "google/gemini-2.0-flash-001" 
    llm_b_model: "meta-llama/llama-3.3-70b-instruct"
    fallback_model: "google/gemini-2.0-flash-lite-preview-02-05:free"
    judge_model: "deepseek/deepseek-r1:free"
    base_url: "https://openrouter.ai/api/v1/chat/completions"

  huggingface:
    model: "meta-llama/Llama-3.3-70B-Instruct" 
    judge_model: "meta-llama/Llama-3.3-70B-Instruct"
    base_url: "https://router.huggingface.co/v1/chat/completions"

# Paths
paths:
  pdf_source: "../data/pdfs/2024-Annual-Report.pdf"
  train_data: "../artifacts/data/train.jsonl"
  test_data: "../artifacts/data/golden_test_set.jsonl"
  intern_preds: "../artifacts/data/intern_predictions.jsonl"
  prompts: "../src/config/prompts.yaml"
  checkpoint_path: "../artifacts/outputs/temp_showdown_progress.csv"
  final_output_path: "../artifacts/outputs/final_showdown.csv"

# Part 1 - Data Factory
roles:
  llm_a: "ollama" #LLMA
  llm_b: "openrouter" #LLMB

chunking:
  size: 1500
  overlap: 0
  separator: " "

generation:
  temperature: 0.7
  request_timeout: 60
  split_ratio: 0.8

# Part 2 - Intern Finetuning
finetuning:
  model_name: "unsloth/llama-3-8b-Instruct-bnb-4bit"
  max_seq_length: 2048
  load_in_4bit: true
  device_map: "cuda:0"

  lora:
    r: 16
    alpha: 16
    dropout: 0
    bias: "none"
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  
  training:
    output_dir: "../artifacts/outputs/llama-3-financial-intern"
    batch_size: 1
    grad_accum_steps: 8
    warmup_steps: 5
    max_steps: 120
    learning_rate: 2.0e-4
    optim: "adamw_8bit"
    seed: 3407

  inference:
    temperature: 0.1
    max_new_tokens: 128
    use_cache: true

# Part 3 - RAG Librarian
weaviate:  
  class_name: "LibrarianChunk"
  vectorizer_model: "sentence-transformers/all-MiniLM-L6-v2"
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

  retrieval:
    method: "explicit_rrf"  # Using manual RRF fusion
    dense_top_k: 20         # Vector search candidates (top_k * 2 in notebook)
    bm25_top_k: 20          # BM25 search candidates (top_k * 2 in notebook)
    rrf_k: 60               # RRF constant for fusion scoring
    final_top_k: 10         # Final results after reranking

# Part 4 - Evaluation Arena
evaluation:
  judge_provider: "ollama" 